{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24611e71-859e-4807-8e32-fc3aa4d4ff25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeremy/miniconda3/envs/finetune/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/jeremy/miniconda3/envs/finetune/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/jeremy/miniconda3/envs/finetune/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/jeremy/miniconda3/envs/finetune/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeremy/miniconda3/envs/finetune/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/jeremy/miniconda3/envs/finetune/lib/libcudart.so'), PosixPath('/home/jeremy/miniconda3/envs/finetune/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.76s/it]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Embedding quantization is only supported with float_qparams_weight_only_qconfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mprepare(trained_model)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Quantize the model (convert to int8)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mquantize_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Save the quantized model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m quantized_output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/llama2/quantized_checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:466\u001b[0m, in \u001b[0;36mquantize_dynamic\u001b[0;34m(model, qconfig_spec, dtype, mapping, inplace)\u001b[0m\n\u001b[1;32m    464\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    465\u001b[0m propagate_qconfig_(model, qconfig_spec)\n\u001b[0;32m--> 466\u001b[0m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:551\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[1;32m    550\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(module)\n\u001b[0;32m--> 551\u001b[0m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[1;32m    555\u001b[0m     _remove_qconfig(module)\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:589\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    588\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[0;32m--> 589\u001b[0m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[1;32m    590\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:591\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    588\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[1;32m    589\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[1;32m    590\u001b[0m                  is_reference, convert_custom_config_dict)\n\u001b[0;32m--> 591\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m \u001b[43mswap_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    594\u001b[0m     module\u001b[38;5;241m.\u001b[39m_modules[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:624\u001b[0m, in \u001b[0;36mswap_module\u001b[0;34m(mod, mapping, custom_module_class_mapping)\u001b[0m\n\u001b[1;32m    622\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod, weight_qparams)\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m \u001b[43mqmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m     swapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m swapped:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# Preserve module's pre forward hooks. They'll be called on quantized input\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/finetune/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/embedding_ops.py:165\u001b[0m, in \u001b[0;36mEmbedding.from_float\u001b[0;34m(cls, mod)\u001b[0m\n\u001b[1;32m    163\u001b[0m dtype \u001b[38;5;241m=\u001b[39m weight_observer\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    164\u001b[0m is_float_qparams_qconfig \u001b[38;5;241m=\u001b[39m weight_observer\u001b[38;5;241m.\u001b[39mqscheme \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mper_channel_affine_float_qparams\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m is_float_qparams_qconfig, \\\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmbedding quantization is only supported with float_qparams_weight_only_qconfig.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mquint8 \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mquint4x2, \\\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe only supported dtype for nnq.Embedding is torch.quint8 and torch.quint4x2, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Run the observer to calculate qparams.\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Embedding quantization is only supported with float_qparams_weight_only_qconfig."
     ]
    }
   ],
   "source": [
    "import torch.quantization\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "output_dir = \"results/llama2/final\"\n",
    "# Load the trained model\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(output_dir)\n",
    "\n",
    "# Quantize the model\n",
    "trained_model.qconfig = torch.quantization.default_qconfig\n",
    "\n",
    "# Prepare the model for quantization\n",
    "trained_model = torch.quantization.prepare(trained_model)\n",
    "\n",
    "# Quantize the model (convert to int8)\n",
    "quantized_model = quantize_dynamic(trained_model)\n",
    "\n",
    "# Save the quantized model\n",
    "quantized_output_dir = \"results/llama2/quantized_checkpoint\"\n",
    "quantized_model.save_pretrained(quantized_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec8943-8ccb-44eb-b95d-a617672a6f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f83d79-1760-4b12-8cf2-589bf902142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e4de5b-3e32-4eaf-aa3c-8505622dc2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59961369-737e-44eb-b56b-0eee68e5a40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1aa4f0-9e34-4650-b60c-751a426a458b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
